{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b76c34",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier√≠a - Computaci√≥n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci√≥n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 7:</strong> Policy Gradients Methods</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern√°ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In√©s Jim√©nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd292cc",
   "metadata": {},
   "source": [
    "## üìù Task 1\n",
    "\n",
    "**Explique la diferencia entre los m√©todos de aprendizaje de refuerzo basados en valores y en pol√≠ticas. ¬øPor qu√© los m√©todos de gradiente de pol√≠ticas son especialmente √∫tiles para entornos con espacios de acci√≥n continua?**\n",
    "\n",
    "En lo que se refiere a los m√©todos value-based, ellos aprenden una funci√≥n de valor y conforme van eligiendo acciones que maximizan dicho valor, tambi√©n van derivando su pol√≠tica. Mientras que los policy-based aprenden directamente de los par√°metros de una pol√≠tica, optimizando as√≠ el retorno esperado con la gradiente de la pol√≠tica. Es debido a esto que los m√©todos de gradiente en pol√≠ticas son √∫tiles en espacios de acci√≥n continua, porque la acci√≥n se muestrea de una distribuci√≥n diferenciable y el entrenamiento ajusta sus par√°metros por la gradiente, evitando maximizar expl√≠citamente sobre un espacio continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0816e",
   "metadata": {},
   "source": [
    "## üìù Task 2\n",
    "\n",
    "**Implemente una versi√≥n simple del algoritmo REINFORCE. Use un entorno simple, como CartPole-v1 de OpenAI Gym, para entrenar a un agente. La funci√≥n de valor estimado debe utilizar un aproximador de funci√≥n lineal.**\n",
    "\n",
    "Pasos a considerar:\n",
    "\n",
    "* Inicialice la pol√≠tica (por ejemplo, una red neuronal o una pol√≠tica softmax).\n",
    "\n",
    "* Simule episodios y obtenga recompensas.\n",
    "\n",
    "* Calcule el rendimiento descontado para cada par de estado-acci√≥n.\n",
    "\n",
    "* Actualice los par√°metros de la pol√≠tica utilizando actualizaciones de gradiente de pol√≠tica.\n",
    "\n",
    "* Agregue una l√≠nea base (funci√≥n de valor estimado) para reducir la varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa7476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(env_id='CartPole-v1', gamma=0.99, policy_lr=0.003, value_lr=0.005, hidden_sizes=(128,), max_episodes=500, max_steps_per_episode=500, reward_goal=475.0, rolling_window=20, log_every=10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYMN = True\n",
    "except Exception:\n",
    "    import gym\n",
    "    GYMN = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    gamma: float = 0.99\n",
    "    policy_lr: float = 3e-3\n",
    "    value_lr: float = 5e-3\n",
    "    hidden_sizes: tuple = (128,)\n",
    "    max_episodes: int = 500\n",
    "    max_steps_per_episode: int = 500\n",
    "    reward_goal: float = 475.0\n",
    "    rolling_window: int = 20\n",
    "    log_every: int = 10\n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe52c9",
   "metadata": {},
   "source": [
    "### Inicializaci√≥n de la pol√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb5e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    # MLP -> acciones. \n",
    "    # Pol√≠tica categ√≥rica con softmax impl√≠cito.\n",
    "    def __init__(self, obs_dim, act_dim, hidden=(128,)):\n",
    "        super().__init__()\n",
    "        layers, d = [], obs_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, act_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ValueLinear(nn.Module):\n",
    "    # Baseline lineal V(s) = w^T s + b\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Linear(obs_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.v(x).squeeze(-1)\n",
    "\n",
    "def initialize_models_and_optimizers(obs_dim, act_dim, cfg: Config):\n",
    "    policy = PolicyNet(obs_dim, act_dim, cfg.hidden).to(DEVICE)\n",
    "    baseline = ValueLinear(obs_dim).to(DEVICE)\n",
    "    opt_pi = optim.Adam(policy.parameters(), lr=cfg.policy_lr)\n",
    "    opt_v  = optim.Adam(baseline.parameters(), lr=cfg.value_lr)\n",
    "    return policy, baseline, opt_pi, opt_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8e1f6",
   "metadata": {},
   "source": [
    "### M√©todo para simulaci√≥n de episodios y obtenci√≥n de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c81ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulate_episode(env, policy, max_steps, device=DEVICE):\n",
    "    if GYMN:\n",
    "        obs, _ = env.reset()\n",
    "    else:\n",
    "        obs = env.reset()\n",
    "\n",
    "    states, actions, logps, rewards = [], [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        s_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        logits = policy(s_t)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        a_t = dist.sample()\n",
    "        logp_t = dist.log_prob(a_t)\n",
    "\n",
    "        step = env.step(a_t.item())\n",
    "        if GYMN:\n",
    "            obs_next, r, term, trunc, _ = step\n",
    "            done = term or trunc\n",
    "        else:\n",
    "            obs_next, r, done, _ = step\n",
    "\n",
    "        states.append(s_t.squeeze(0).cpu().numpy())\n",
    "        actions.append(a_t.item())\n",
    "        logps.append(logp_t.item())\n",
    "        rewards.append(float(r))\n",
    "\n",
    "        obs = obs_next\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, logps, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b88a2b",
   "metadata": {},
   "source": [
    "### C√°lculo del rendimiento descontado para cada estado-acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqu√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c1794",
   "metadata": {},
   "source": [
    "### Actualizaci√≥n de los par√°metros de la pol√≠tica utilizando actualizaciones de gradiente de pol√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqu√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7fa76",
   "metadata": {},
   "source": [
    "### Agregaci√≥n de l√≠nea base/funci√≥n de valor estimado para reducci√≥n de la varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqu√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930d7f7",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqu√≠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
