{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b76c34",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier√≠a - Computaci√≥n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci√≥n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 7:</strong> Policy Gradients Methods</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern√°ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In√©s Jim√©nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd292cc",
   "metadata": {},
   "source": [
    "## üìù Task 1\n",
    "\n",
    "**Explique la diferencia entre los m√©todos de aprendizaje de refuerzo basados en valores y en pol√≠ticas. ¬øPor qu√© los m√©todos de gradiente de pol√≠ticas son especialmente √∫tiles para entornos con espacios de acci√≥n continua?**\n",
    "\n",
    "En lo que se refiere a los m√©todos value-based, ellos aprenden una funci√≥n de valor y conforme van eligiendo acciones que maximizan dicho valor, tambi√©n van derivando su pol√≠tica. Mientras que los policy-based aprenden directamente de los par√°metros de una pol√≠tica, optimizando as√≠ el retorno esperado con la gradiente de la pol√≠tica. Es debido a esto que los m√©todos de gradiente en pol√≠ticas son √∫tiles en espacios de acci√≥n continua, porque la acci√≥n se muestrea de una distribuci√≥n diferenciable y el entrenamiento ajusta sus par√°metros por la gradiente, evitando maximizar expl√≠citamente sobre un espacio continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0816e",
   "metadata": {},
   "source": [
    "## üìù Task 2\n",
    "\n",
    "**Implemente una versi√≥n simple del algoritmo REINFORCE. Use un entorno simple, como CartPole-v1 de OpenAI Gym, para entrenar a un agente. La funci√≥n de valor estimado debe utilizar un aproximador de funci√≥n lineal.**\n",
    "\n",
    "Pasos a considerar:\n",
    "\n",
    "* Inicialice la pol√≠tica (por ejemplo, una red neuronal o una pol√≠tica softmax).\n",
    "\n",
    "* Simule episodios y obtenga recompensas.\n",
    "\n",
    "* Calcule el rendimiento descontado para cada par de estado-acci√≥n.\n",
    "\n",
    "* Actualice los par√°metros de la pol√≠tica utilizando actualizaciones de gradiente de pol√≠tica.\n",
    "\n",
    "* Agregue una l√≠nea base (funci√≥n de valor estimado) para reducir la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a0b9b",
   "metadata": {},
   "source": [
    "# Librer√≠as + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35474d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import numpy as np\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe52c9",
   "metadata": {},
   "source": [
    "### Inicializaci√≥n de la pol√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obsDim, actDim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obsDim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, actDim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def initPolicy(envId=\"CartPole-v1\", policyLr=3e-3):\n",
    "    env = gym.make(envId)\n",
    "    obsDim = env.observation_space.shape[0] # type: ignore\n",
    "    actDim = env.action_space.n # type: ignore\n",
    "    policy = PolicyNet(obsDim, actDim).to(DEVICE)\n",
    "    optPi  = optim.Adam(policy.parameters(), lr=policyLr)\n",
    "    return env, policy, optPi, obsDim, actDim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8e1f6",
   "metadata": {},
   "source": [
    "### M√©todo para simulaci√≥n de episodios y obtenci√≥n de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5c81ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulateEpisode(env, policy, maxSteps=500):\n",
    "    obs, _ = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    for _ in range(maxSteps):\n",
    "        s = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        logits = policy(s)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        a = dist.sample()\n",
    "        obs, r, terminated, truncated, _ = env.step(a.item())\n",
    "        done = terminated or truncated  \n",
    "        states.append(s.squeeze(0).cpu().numpy())\n",
    "        actions.append(a.item())\n",
    "        rewards.append(float(r))\n",
    "        if done: break\n",
    "\n",
    "    return np.array(states, np.float32), np.array(actions), np.array(rewards, np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b88a2b",
   "metadata": {},
   "source": [
    "### C√°lculo del rendimiento descontado para cada estado-acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDiscountedReturns(rewards, gamma=0.99):\n",
    "    T = len(rewards)\n",
    "    G = np.zeros(T, dtype=np.float32)\n",
    "    running = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        running = rewards[t] + gamma * running\n",
    "        G[t] = running\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c1794",
   "metadata": {},
   "source": [
    "### Actualizaci√≥n de los par√°metros de la pol√≠tica utilizando actualizaciones de gradiente de pol√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePolicy(policy, optPi, statesNp, actionsNp, advantagesT):\n",
    "    statesT  = torch.tensor(statesNp,  dtype=torch.float32, device=DEVICE)\n",
    "    actionsT = torch.tensor(actionsNp, dtype=torch.int64,   device=DEVICE)\n",
    "\n",
    "    dist = torch.distributions.Categorical(logits=policy(statesT))\n",
    "    logProbs = dist.log_prob(actionsT)\n",
    "    lossPi = -(logProbs * advantagesT.detach()).mean()\n",
    "\n",
    "    optPi.zero_grad()\n",
    "    lossPi.backward()\n",
    "    optPi.step()\n",
    "    return float(lossPi.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7fa76",
   "metadata": {},
   "source": [
    "### Agregaci√≥n de l√≠nea base/funci√≥n de valor estimado para reducci√≥n de la varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueLinear(nn.Module):\n",
    "    \"\"\"Baseline lineal: V(s) = w^T s + b.\"\"\"\n",
    "    def __init__(self, obsDim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Linear(obsDim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.v(x).squeeze(-1)\n",
    "\n",
    "def initBaseline(obsDim, valueLr=5e-3):\n",
    "    valueFn = ValueLinear(obsDim).to(DEVICE)\n",
    "    optV = optim.Adam(valueFn.parameters(), lr=valueLr)\n",
    "    return valueFn, optV\n",
    "\n",
    "def computeAdvantages(valueFn, statesNp, returnsNp):\n",
    "    with torch.no_grad():\n",
    "        vals = valueFn(torch.tensor(statesNp, dtype=torch.float32, device=DEVICE))\n",
    "    adv = torch.tensor(returnsNp, device=DEVICE) - vals\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "    return adv\n",
    "\n",
    "def updateValue(valueFn, optV, statesNp, returnsNp):\n",
    "    statesT  = torch.tensor(statesNp,  dtype=torch.float32, device=DEVICE)\n",
    "    returnsT = torch.tensor(returnsNp, dtype=torch.float32, device=DEVICE)\n",
    "    pred = valueFn(statesT)\n",
    "    lossV = ((pred - returnsT) ** 2).mean()\n",
    "\n",
    "    optV.zero_grad()\n",
    "    lossV.backward()\n",
    "    optV.step()\n",
    "    return float(lossV.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930d7f7",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "569cbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainReinforce(envId=\"CartPole-v1\", gamma=0.99, maxEpisodes=750, maxSteps=500, logEvery=10):\n",
    "    env, policy, optPi, obsDim, actDim = initPolicy(envId)\n",
    "    valueFn, optV = initBaseline(obsDim)\n",
    "    history = []\n",
    "\n",
    "    for ep in range(1, maxEpisodes+1):\n",
    "        states, actions, rewards = simulateEpisode(env, policy, maxSteps)\n",
    "        returnsNp = computeDiscountedReturns(rewards, gamma)\n",
    "        advantagesT = computeAdvantages(valueFn, states, returnsNp)\n",
    "\n",
    "        lossPi = updatePolicy(policy, optPi, states, actions, advantagesT)\n",
    "        lossV  = updateValue(valueFn, optV, states, returnsNp)\n",
    "\n",
    "        epReturn = float(rewards.sum()); history.append(epReturn)\n",
    "        if ep % logEvery == 0:\n",
    "            avg20 = np.mean(history[-20:]) if len(history) >= 20 else np.mean(history)\n",
    "            print(f\"EPISODE {ep:4d} - RETURN: {epReturn:6.1f} - AVG: {avg20:6.1f} - LœÄ: {lossPi:.4f} - Lv: {lossV:.4f}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e9462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE   10 - RETURN:   13.0 - AVG:   27.6 - LœÄ: 0.0306 - Lv: 53.0284\n",
      "EPISODE   20 - RETURN:   37.0 - AVG:   28.3 - LœÄ: 0.0042 - Lv: 347.7358\n",
      "EPISODE   30 - RETURN:   21.0 - AVG:   36.3 - LœÄ: 0.0272 - Lv: 125.1768\n",
      "EPISODE   40 - RETURN:   22.0 - AVG:   43.2 - LœÄ: 0.0383 - Lv: 134.8622\n",
      "EPISODE   50 - RETURN:   92.0 - AVG:   43.3 - LœÄ: -0.0354 - Lv: 1474.0721\n",
      "EPISODE   60 - RETURN:   38.0 - AVG:   47.5 - LœÄ: 0.0153 - Lv: 356.6764\n",
      "EPISODE   70 - RETURN:  116.0 - AVG:   55.5 - LœÄ: 0.0296 - Lv: 2016.9814\n",
      "EPISODE   80 - RETURN:   39.0 - AVG:   60.6 - LœÄ: 0.0241 - Lv: 368.4306\n",
      "EPISODE   90 - RETURN:   99.0 - AVG:   74.8 - LœÄ: 0.0210 - Lv: 1601.1178\n",
      "EPISODE  100 - RETURN:  107.0 - AVG:  105.2 - LœÄ: 0.0024 - Lv: 1780.9792\n",
      "EPISODE  110 - RETURN:  177.0 - AVG:  115.3 - LœÄ: 0.0002 - Lv: 3303.7903\n",
      "EPISODE  120 - RETURN:  169.0 - AVG:  134.4 - LœÄ: 0.0092 - Lv: 3147.3569\n",
      "EPISODE  130 - RETURN:  260.0 - AVG:  186.9 - LœÄ: -0.0013 - Lv: 4630.5469\n",
      "EPISODE  140 - RETURN:  219.0 - AVG:  223.2 - LœÄ: 0.0129 - Lv: 3990.6531\n",
      "EPISODE  150 - RETURN:  146.0 - AVG:  225.1 - LœÄ: 0.0069 - Lv: 2613.3142\n",
      "EPISODE  160 - RETURN:  204.0 - AVG:  222.5 - LœÄ: 0.0040 - Lv: 3714.2341\n",
      "EPISODE  170 - RETURN:  235.0 - AVG:  206.1 - LœÄ: -0.0001 - Lv: 4207.3608\n",
      "EPISODE  180 - RETURN:  211.0 - AVG:  204.2 - LœÄ: 0.0157 - Lv: 3762.3628\n",
      "EPISODE  190 - RETURN:  143.0 - AVG:  277.2 - LœÄ: -0.0175 - Lv: 2533.6099\n",
      "EPISODE  200 - RETURN:  500.0 - AVG:  325.6 - LœÄ: 0.0085 - Lv: 6742.8389\n",
      "EPISODE  210 - RETURN:  500.0 - AVG:  339.1 - LœÄ: -0.0223 - Lv: 6707.9429\n",
      "EPISODE  220 - RETURN:  500.0 - AVG:  388.1 - LœÄ: -0.0254 - Lv: 6625.8906\n",
      "EPISODE  230 - RETURN:  489.0 - AVG:  450.6 - LœÄ: -0.0070 - Lv: 6752.5493\n",
      "EPISODE  240 - RETURN:  500.0 - AVG:  476.3 - LœÄ: -0.0046 - Lv: 6632.7622\n",
      "EPISODE  250 - RETURN:  500.0 - AVG:  479.3 - LœÄ: 0.0100 - Lv: 6826.7129\n",
      "EPISODE  260 - RETURN:  242.0 - AVG:  453.9 - LœÄ: -0.0322 - Lv: 4377.9990\n",
      "EPISODE  270 - RETURN:  500.0 - AVG:  437.3 - LœÄ: -0.0080 - Lv: 6619.9141\n",
      "EPISODE  280 - RETURN:  206.0 - AVG:  410.8 - LœÄ: 0.0075 - Lv: 3579.2695\n",
      "EPISODE  290 - RETURN:  292.0 - AVG:  329.1 - LœÄ: 0.0078 - Lv: 4804.1562\n",
      "EPISODE  300 - RETURN:  500.0 - AVG:  321.3 - LœÄ: -0.0355 - Lv: 6575.5586\n",
      "EPISODE  310 - RETURN:  500.0 - AVG:  392.4 - LœÄ: -0.0121 - Lv: 6696.7212\n",
      "EPISODE  320 - RETURN:  409.0 - AVG:  406.6 - LœÄ: -0.0029 - Lv: 6095.9805\n",
      "EPISODE  330 - RETURN:  137.0 - AVG:  330.7 - LœÄ: 0.0083 - Lv: 2275.0471\n",
      "EPISODE  340 - RETURN:   58.0 - AVG:  197.2 - LœÄ: -0.0669 - Lv: 640.8543\n",
      "EPISODE  350 - RETURN:  105.0 - AVG:  100.5 - LœÄ: 0.0005 - Lv: 1560.8789\n",
      "EPISODE  360 - RETURN:  138.0 - AVG:   99.8 - LœÄ: -0.0095 - Lv: 2256.4258\n",
      "EPISODE  370 - RETURN:  108.0 - AVG:  120.2 - LœÄ: 0.0278 - Lv: 1597.0887\n",
      "EPISODE  380 - RETURN:  137.0 - AVG:  133.0 - LœÄ: -0.0141 - Lv: 2197.2495\n",
      "EPISODE  390 - RETURN:  246.0 - AVG:  173.2 - LœÄ: -0.0052 - Lv: 4049.0269\n",
      "EPISODE  400 - RETURN:  500.0 - AVG:  340.8 - LœÄ: -0.0315 - Lv: 6325.5278\n",
      "EPISODE  410 - RETURN:  248.0 - AVG:  421.4 - LœÄ: 0.0202 - Lv: 4058.4583\n",
      "EPISODE  420 - RETURN:  128.0 - AVG:  262.4 - LœÄ: 0.0157 - Lv: 1963.6322\n",
      "EPISODE  430 - RETURN:  261.0 - AVG:  183.0 - LœÄ: 0.0045 - Lv: 4166.1265\n",
      "EPISODE  440 - RETURN:  476.0 - AVG:  286.1 - LœÄ: 0.0214 - Lv: 6038.5098\n",
      "EPISODE  450 - RETURN:  500.0 - AVG:  417.1 - LœÄ: -0.0059 - Lv: 6263.1646\n",
      "EPISODE  460 - RETURN:  500.0 - AVG:  446.9 - LœÄ: 0.0191 - Lv: 6283.0908\n",
      "EPISODE  470 - RETURN:  485.0 - AVG:  438.1 - LœÄ: -0.0244 - Lv: 6341.3579\n",
      "EPISODE  480 - RETURN:  500.0 - AVG:  460.8 - LœÄ: 0.0124 - Lv: 6525.2529\n",
      "EPISODE  490 - RETURN:  500.0 - AVG:  476.4 - LœÄ: -0.0035 - Lv: 6611.3931\n",
      "EPISODE  500 - RETURN:  500.0 - AVG:  493.9 - LœÄ: -0.0408 - Lv: 6494.7832\n",
      "EPISODE  510 - RETURN:  500.0 - AVG:  485.1 - LœÄ: 0.0197 - Lv: 6362.3193\n",
      "EPISODE  520 - RETURN:  500.0 - AVG:  457.9 - LœÄ: 0.0045 - Lv: 6320.2402\n",
      "EPISODE  530 - RETURN:  500.0 - AVG:  472.8 - LœÄ: -0.0208 - Lv: 6184.1689\n",
      "EPISODE  540 - RETURN:  500.0 - AVG:  496.1 - LœÄ: -0.0043 - Lv: 5954.9692\n",
      "EPISODE  550 - RETURN:  500.0 - AVG:  464.6 - LœÄ: 0.0135 - Lv: 5931.5806\n",
      "EPISODE  560 - RETURN:  500.0 - AVG:  453.4 - LœÄ: -0.0264 - Lv: 5987.3105\n",
      "EPISODE  570 - RETURN:  484.0 - AVG:  475.6 - LœÄ: 0.0043 - Lv: 6114.2261\n",
      "EPISODE  580 - RETURN:  500.0 - AVG:  490.7 - LœÄ: 0.0051 - Lv: 5973.1597\n",
      "EPISODE  590 - RETURN:  500.0 - AVG:  491.6 - LœÄ: 0.0274 - Lv: 5865.1094\n",
      "EPISODE  600 - RETURN:  344.0 - AVG:  483.8 - LœÄ: -0.0274 - Lv: 4766.3887\n",
      "EPISODE  610 - RETURN:  500.0 - AVG:  468.1 - LœÄ: -0.0092 - Lv: 5832.4170\n",
      "EPISODE  620 - RETURN:  121.0 - AVG:  398.8 - LœÄ: 0.0083 - Lv: 1674.3514\n",
      "EPISODE  630 - RETURN:  119.0 - AVG:  299.6 - LœÄ: 0.0182 - Lv: 1601.5902\n",
      "EPISODE  640 - RETURN:  126.0 - AVG:  203.5 - LœÄ: -0.0114 - Lv: 1819.5050\n",
      "EPISODE  650 - RETURN:  310.0 - AVG:  152.9 - LœÄ: -0.0254 - Lv: 4233.5303\n",
      "EPISODE  660 - RETURN:  500.0 - AVG:  252.5 - LœÄ: -0.0083 - Lv: 5844.1299\n",
      "EPISODE  670 - RETURN:  245.0 - AVG:  372.9 - LœÄ: -0.0461 - Lv: 3746.0791\n",
      "EPISODE  680 - RETURN:  122.0 - AVG:  348.2 - LœÄ: -0.0059 - Lv: 1668.5807\n",
      "EPISODE  690 - RETURN:  142.0 - AVG:  251.8 - LœÄ: -0.0139 - Lv: 2127.6328\n",
      "EPISODE  700 - RETURN:  109.0 - AVG:  153.7 - LœÄ: 0.0061 - Lv: 1459.1376\n",
      "EPISODE  710 - RETURN:  113.0 - AVG:  103.5 - LœÄ: -0.0144 - Lv: 1551.7815\n",
      "EPISODE  720 - RETURN:  118.0 - AVG:  107.2 - LœÄ: 0.0075 - Lv: 1651.4158\n",
      "EPISODE  730 - RETURN:  122.0 - AVG:  104.8 - LœÄ: -0.0676 - Lv: 1723.1517\n",
      "EPISODE  740 - RETURN:   96.0 - AVG:  103.6 - LœÄ: 0.0325 - Lv: 1158.2194\n",
      "EPISODE  750 - RETURN:   39.0 - AVG:   85.6 - LœÄ: -0.1145 - Lv: 248.4950\n"
     ]
    }
   ],
   "source": [
    "trainReinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944833ab",
   "metadata": {},
   "source": [
    "Los resultados reflejan el proceso de aprendizaje que realiza el agente mediante REINFORCE con baseline en el entorno de CartPole. Al inicio, los retornos fueron bajos e inestables, lo que muestra c√≥mo se comporta la fase de exploraci√≥n (inicial) en la que la pol√≠tica a√∫n era casi aleatoria. Con el paso de los episodios, los promedios comenzaron a incrementarse hasta alcanzar valores cercanos al m√°ximo posible (500), evidenciando que la pol√≠tica logr√≥ aprender a mantener el equilibrio del palo de manera consistente. Aunque se observan ca√≠das puntuales, el desempe√±o general confirma que el agente alcanz√≥ y mantuvo el criterio de ‚Äúresolver‚Äù el entorno, estabilizando su comportamiento a trav√©s de la retroalimentaci√≥n basada en retornos y la correcci√≥n de varianza mediante la funci√≥n de valor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
